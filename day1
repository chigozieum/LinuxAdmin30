### Bash Scripting and Logging Mastery: A Comprehensive Guide for Linux Engineers

## Introduction: Why Mastering Bash and Logging Is Your Secret Weapon

Welcome to Day 1 of our 30-Day Linux Engineer Training Series. Today, we're diving deep into the foundational skills that separate amateur scripters from professional Linux engineers: Bash scripting and logging.

In the world of Linux administration and DevOps, your ability to write reliable, maintainable scripts is directly proportional to your value as an engineer. Scripts without proper logging are like cars without dashboards—you might be moving, but you have no idea what's happening under the hood.

### The Value Proposition of Professional Logging

Before we write a single line of code, let's understand why logging transforms good scripts into great ones:

- **Observability**: Logs provide visibility into what your script is doing at each step
- **Troubleshooting**: When things go wrong (and they will), logs are your first line of defense
- **Accountability**: Logs create an audit trail of actions and changes
- **Confidence**: Well-logged scripts inspire trust from team members and stakeholders
- **Scalability**: As your infrastructure grows, proper logging becomes not just useful but essential


Let's begin our journey to mastering these critical skills.

## Part 1: The Anatomy of a Professional Bash Script

### The Foundation: Shebang and Script Structure

Every professional Bash script starts with a proper structure:

```shellscript
#!/bin/bash
#
# Script Name: example_script.sh
# Description: Demonstrates professional script structure and logging
# Author: Your Name
# Date Created: 2025-04-04
# Last Modified: 2025-04-04
#
# Usage: ./example_script.sh [options]
#
# Exit Codes:
#   0 - Success
#   1 - General error
#   2 - Missing dependency
#   3 - Permission denied
#
# Dependencies: date, grep, awk

# Script version
VERSION="1.0.0"

# Source configuration if exists
if [[ -f "./config.sh" ]]; then
    source "./config.sh"
fi

# Main code begins here
```

Let's break down these components:

1. **Shebang (`#!/bin/bash`)**: Tells the system to use Bash as the interpreter
2. **Header Comment Block**: Documents the script's purpose, author, and usage
3. **Version Tracking**: Makes it easy to identify which version is running
4. **Configuration Sourcing**: Separates configuration from code
5. **Exit Code Documentation**: Defines what each exit code means


This structure immediately communicates professionalism and attention to detail.

### Variables and Constants: The Building Blocks

Variables in Bash scripts should follow these best practices:

```shellscript
# Constants (use uppercase)
readonly MAX_RETRIES=5
readonly BACKUP_DIR="/var/backups"

# Variables (use lowercase)
current_date=$(date +%Y-%m-%d)
user_count=0

# Arrays
declare -a users=("admin" "developer" "tester")
```

**Pro Tips for Variables:**

- Use `readonly` for constants to prevent accidental modification
- Always quote your variables: `"$variable"` not just `$variable`
- Use meaningful names that describe the purpose
- For complex data, consider using arrays or associative arrays


## Part 2: Logging Framework - Building Your Script's Nervous System

### Basic Logging Function

Let's start with a simple but effective logging function:

```shellscript
log_message() {
    local log_level="$1"
    local message="$2"
    local timestamp=$(date '+%Y-%m-%d %H:%M:%S')
    
    echo "[$timestamp] [$log_level] $message"
}

# Usage examples
log_message "INFO" "Script started"
log_message "WARNING" "Disk space is below 20%"
log_message "ERROR" "Failed to connect to database"
```

This function adds:

- A timestamp for each message
- A log level to indicate severity
- A consistent format for all log entries


### Advanced Logging: Adding Colors and Log Levels

Let's enhance our logging with colors and proper log levels:

```shellscript
# Define log levels
readonly LOG_LEVEL_DEBUG=0
readonly LOG_LEVEL_INFO=1
readonly LOG_LEVEL_WARNING=2
readonly LOG_LEVEL_ERROR=3
readonly LOG_LEVEL_CRITICAL=4

# Current log level (can be overridden via command line)
CURRENT_LOG_LEVEL=$LOG_LEVEL_INFO

# ANSI color codes
readonly COLOR_RESET="\033[0m"
readonly COLOR_RED="\033[0;31m"
readonly COLOR_GREEN="\033[0;32m"
readonly COLOR_YELLOW="\033[0;33m"
readonly COLOR_BLUE="\033[0;34m"
readonly COLOR_PURPLE="\033[0;35m"

log() {
    local level=$1
    local message=$2
    local timestamp=$(date '+%Y-%m-%d %H:%M:%S')
    
    # Only log if the current log level is less than or equal to the message level
    if [[ $level -ge $CURRENT_LOG_LEVEL ]]; then
        case $level in
            $LOG_LEVEL_DEBUG)
                echo -e "[$timestamp] ${COLOR_BLUE}[DEBUG]${COLOR_RESET} $message"
                ;;
            $LOG_LEVEL_INFO)
                echo -e "[$timestamp] ${COLOR_GREEN}[INFO]${COLOR_RESET} $message"
                ;;
            $LOG_LEVEL_WARNING)
                echo -e "[$timestamp] ${COLOR_YELLOW}[WARNING]${COLOR_RESET} $message"
                ;;
            $LOG_LEVEL_ERROR)
                echo -e "[$timestamp] ${COLOR_RED}[ERROR]${COLOR_RESET} $message"
                ;;
            $LOG_LEVEL_CRITICAL)
                echo -e "[$timestamp] ${COLOR_PURPLE}[CRITICAL]${COLOR_RESET} $message"
                ;;
        esac
    fi
}

# Convenience functions
debug() { log $LOG_LEVEL_DEBUG "$1"; }
info() { log $LOG_LEVEL_INFO "$1"; }
warning() { log $LOG_LEVEL_WARNING "$1"; }
error() { log $LOG_LEVEL_ERROR "$1"; }
critical() { log $LOG_LEVEL_CRITICAL "$1"; }
```

This advanced logging framework provides:

1. **Log Levels**: Filter logs based on severity
2. **Color Coding**: Visual differentiation of log types
3. **Convenience Functions**: Simplified logging calls


### Logging to Files and Console Simultaneously

For production scripts, you often want to log to both the console and a file:

```shellscript
# Log file setup
LOG_DIR="/var/log/scripts"
LOG_FILE="$LOG_DIR/$(basename "$0" .sh)_$(date +%Y%m%d).log"

# Create log directory if it doesn't exist
mkdir -p "$LOG_DIR"

log_to_file_and_console() {
    local level=$1
    local message=$2
    local timestamp=$(date '+%Y-%m-%d %H:%M:%S')
    
    # Format the log message
    case $level in
        $LOG_LEVEL_DEBUG)
            formatted_message="[$timestamp] [DEBUG] $message"
            console_message="[$timestamp] ${COLOR_BLUE}[DEBUG]${COLOR_RESET} $message"
            ;;
        $LOG_LEVEL_INFO)
            formatted_message="[$timestamp] [INFO] $message"
            console_message="[$timestamp] ${COLOR_GREEN}[INFO]${COLOR_RESET} $message"
            ;;
        $LOG_LEVEL_WARNING)
            formatted_message="[$timestamp] [WARNING] $message"
            console_message="[$timestamp] ${COLOR_YELLOW}[WARNING]${COLOR_RESET} $message"
            ;;
        $LOG_LEVEL_ERROR)
            formatted_message="[$timestamp] [ERROR] $message"
            console_message="[$timestamp] ${COLOR_RED}[ERROR]${COLOR_RESET} $message"
            ;;
        $LOG_LEVEL_CRITICAL)
            formatted_message="[$timestamp] [CRITICAL] $message"
            console_message="[$timestamp] ${COLOR_PURPLE}[CRITICAL]${COLOR_RESET} $message"
            ;;
    esac
    
    # Log to file (without colors)
    echo "$formatted_message" >> "$LOG_FILE"
    
    # Log to console (with colors)
    echo -e "$console_message"
}

# Update convenience functions
debug() { [[ $LOG_LEVEL_DEBUG -ge $CURRENT_LOG_LEVEL ]] && log_to_file_and_console $LOG_LEVEL_DEBUG "$1"; }
info() { [[ $LOG_LEVEL_INFO -ge $CURRENT_LOG_LEVEL ]] && log_to_file_and_console $LOG_LEVEL_INFO "$1"; }
warning() { [[ $LOG_LEVEL_WARNING -ge $CURRENT_LOG_LEVEL ]] && log_to_file_and_console $LOG_LEVEL_WARNING "$1"; }
error() { [[ $LOG_LEVEL_ERROR -ge $CURRENT_LOG_LEVEL ]] && log_to_file_and_console $LOG_LEVEL_ERROR "$1"; }
critical() { [[ $LOG_LEVEL_CRITICAL -ge $CURRENT_LOG_LEVEL ]] && log_to_file_and_console $LOG_LEVEL_CRITICAL "$1"; }
```

This enhanced logging system:

- Creates dated log files
- Writes plain text to files (for easier parsing)
- Shows colored output in the terminal
- Maintains the same log level filtering


## Part 3: Error Handling - The Safety Net

### Basic Error Handling

Error handling is where many scripts fall short. Let's implement a robust error handling system:

```shellscript
handle_error() {
    local exit_code=$1
    local error_message=$2
    local line_number=$3
    
    # Log the error with file and line information
    error "Error on line $line_number: $error_message (Exit Code: $exit_code)"
    
    # Exit with the provided code
    exit "$exit_code"
}

# Set up trap to catch errors
trap 'handle_error $? "Command failed" $LINENO' ERR
```

### Command Result Checking

Always check the result of critical commands:

```shellscript
execute_with_retry() {
    local cmd="$1"
    local max_attempts=${2:-3}
    local attempt=1
    local result=0
    
    while [[ $attempt -le $max_attempts ]]; do
        debug "Attempt $attempt/$max_attempts: $cmd"
        
        # Execute the command
        eval "$cmd"
        result=$?
        
        if [[ $result -eq 0 ]]; then
            info "Command succeeded on attempt $attempt"
            return 0
        else
            warning "Command failed on attempt $attempt with exit code $result"
            attempt=$((attempt + 1))
            
            if [[ $attempt -le $max_attempts ]]; then
                sleep 2  # Wait before retrying
            fi
        fi
    done
    
    error "Command failed after $max_attempts attempts: $cmd"
    return $result
}

# Usage example
execute_with_retry "rsync -avz /source/ /destination/" 5
```

This function:

- Attempts a command multiple times before giving up
- Logs each attempt and its result
- Implements exponential backoff (with the sleep)
- Returns the final exit code


## Part 4: Real-World Examples - Putting It All Together

### Example 1: System Backup Script

Let's create a comprehensive backup script that demonstrates our logging and error handling:

```shellscript
#!/bin/bash
#
# Script Name: system_backup.sh
# Description: Creates a compressed backup of specified directories
# Author: Your Name
# Date Created: 2025-04-04
#
# Usage: ./system_backup.sh [--debug]

# Script setup
set -o pipefail  # Ensure pipeline failures are caught

# Source our logging library (assuming we've saved the above code as logging.sh)
source "./logging.sh"

# Configuration
readonly BACKUP_ROOT="/var/backups"
readonly BACKUP_DIRS=("/etc" "/home" "/var/www")
readonly BACKUP_NAME="system_backup_$(date +%Y%m%d_%H%M%S).tar.gz"
readonly BACKUP_PATH="$BACKUP_ROOT/$BACKUP_NAME"
readonly MAX_BACKUPS=5

# Process command line arguments
for arg in "$@"; do
    case $arg in
        --debug)
            CURRENT_LOG_LEVEL=$LOG_LEVEL_DEBUG
            debug "Debug mode enabled"
            ;;
    esac
done

# Check if running as root
if [[ $EUID -ne 0 ]]; then
    critical "This script must be run as root"
    exit 1
fi

# Create backup directory if it doesn't exist
info "Setting up backup directory"
if ! mkdir -p "$BACKUP_ROOT"; then
    error "Failed to create backup directory: $BACKUP_ROOT"
    exit 1
fi

# Check available disk space
check_disk_space() {
    local required_space=$1  # in MB
    local available_space
    
    available_space=$(df -m "$BACKUP_ROOT" | awk 'NR==2 {print $4}')
    debug "Available space: ${available_space}MB, Required: ${required_space}MB"
    
    if [[ $available_space -lt $required_space ]]; then
        warning "Low disk space: ${available_space}MB available, ${required_space}MB recommended"
        return 1
    fi
    
    return 0
}

# Check if we have enough space (estimate 500MB)
if ! check_disk_space 500; then
    warning "Proceeding with backup despite low disk space"
fi

# Perform the backup
info "Starting backup of: ${BACKUP_DIRS[*]}"
if tar -czf "$BACKUP_PATH" "${BACKUP_DIRS[@]}" 2> >(while read -r line; do debug "tar: $line"; done); then
    info "Backup created successfully: $BACKUP_PATH"
    
    # Get backup size
    backup_size=$(du -h "$BACKUP_PATH" | cut -f1)
    info "Backup size: $backup_size"
else
    error "Backup creation failed with exit code $?"
    exit 1
fi

# Rotate old backups
rotate_old_backups() {
    local backup_count
    backup_count=$(find "$BACKUP_ROOT" -name "system_backup_*.tar.gz" | wc -l)
    
    if [[ $backup_count -gt $MAX_BACKUPS ]]; then
        info "Rotating old backups, keeping last $MAX_BACKUPS"
        find "$BACKUP_ROOT" -name "system_backup_*.tar.gz" -type f -printf "%T@ %p\n" | \
            sort -n | head -n -"$MAX_BACKUPS" | cut -d' ' -f2- | \
            while read -r old_backup; do
                debug "Removing old backup: $old_backup"
                rm "$old_backup"
            done
    else
        debug "No backup rotation needed ($backup_count backups exist)"
    fi
}

# Rotate old backups
rotate_old_backups

# Verify backup integrity
info "Verifying backup integrity"
if ! tar -tzf "$BACKUP_PATH" > /dev/null 2>&1; then
    critical "Backup verification failed! The backup may be corrupted."
    exit 1
fi

info "Backup process completed successfully"
exit 0
```

This script demonstrates:

- Comprehensive logging at different levels
- Proper error handling and exit codes
- Command-line argument processing
- Disk space checking
- Backup rotation
- Verification of backup integrity


### Example 2: Service Monitoring Script

Here's another real-world example that monitors critical services and restarts them if needed:

```shellscript
#!/bin/bash
#
# Script Name: service_monitor.sh
# Description: Monitors critical services and restarts them if they're down
# Author: Your Name
# Date Created: 2025-04-04
#
# Usage: ./service_monitor.sh [--notify-email email@example.com]

# Source our logging library
source "./logging.sh"

# Configuration
readonly SERVICES=("nginx" "postgresql" "redis-server" "elasticsearch")
readonly CHECK_INTERVAL=300  # seconds
readonly MAX_RESTART_ATTEMPTS=3
readonly NOTIFICATION_EMAIL=""  # Default, can be overridden

# Process command line arguments
while [[ $# -gt 0 ]]; do
    case $1 in
        --notify-email)
            NOTIFICATION_EMAIL="$2"
            info "Email notifications will be sent to: $NOTIFICATION_EMAIL"
            shift 2
            ;;
        *)
            warning "Unknown argument: $1"
            shift
            ;;
    esac
done

# Function to check if a service is running
is_service_running() {
    local service_name="$1"
    
    if systemctl is-active --quiet "$service_name"; then
        debug "Service $service_name is running"
        return 0
    else
        warning "Service $service_name is not running"
        return 1
    fi
}

# Function to restart a service
restart_service() {
    local service_name="$1"
    
    info "Attempting to restart $service_name"
    if systemctl restart "$service_name"; then
        info "Successfully restarted $service_name"
        return 0
    else
        error "Failed to restart $service_name"
        return 1
    fi
}

# Function to send email notification
send_notification() {
    local subject="$1"
    local message="$2"
    
    if [[ -z "$NOTIFICATION_EMAIL" ]]; then
        debug "No notification email configured, skipping notification"
        return 0
    fi
    
    debug "Sending notification email to $NOTIFICATION_EMAIL"
    if echo -e "$message" | mail -s "$subject" "$NOTIFICATION_EMAIL"; then
        debug "Email notification sent successfully"
        return 0
    else
        warning "Failed to send email notification"
        return 1
    fi
}

# Main monitoring loop
info "Starting service monitoring"
info "Monitoring services: ${SERVICES[*]}"
info "Check interval: $CHECK_INTERVAL seconds"

while true; do
    for service in "${SERVICES[@]}"; do
        if ! is_service_running "$service"; then
            warning "Service $service is down, attempting to restart"
            
            # Try to restart the service
            restart_attempts=0
            while [[ $restart_attempts -lt $MAX_RESTART_ATTEMPTS ]]; do
                restart_attempts=$((restart_attempts + 1))
                
                if restart_service "$service"; then
                    # Check if it's actually running after restart
                    if is_service_running "$service"; then
                        info "Service $service successfully restarted on attempt $restart_attempts"
                        
                        # Send notification about successful restart
                        send_notification "Service Restarted: $service" \
                            "The service $service was down and has been successfully restarted.\n\nTimestamp: $(date)\nServer: $(hostname)\nAttempt: $restart_attempts"
                        
                        break
                    else
                        warning "Service $service still not running after restart attempt $restart_attempts"
                    fi
                fi
                
                # If we've reached max attempts and still failed
                if [[ $restart_attempts -eq $MAX_RESTART_ATTEMPTS ]]; then
                    critical "Failed to restart $service after $MAX_RESTART_ATTEMPTS attempts"
                    
                    # Send notification about failed restart
                    send_notification "CRITICAL: Service $service Down" \
                        "The service $service is down and could not be restarted after $MAX_RESTART_ATTEMPTS attempts.\n\nTimestamp: $(date)\nServer: $(hostname)\nPlease investigate immediately."
                else
                    # Wait before next attempt
                    sleep 5
                fi
            done
        fi
    done
    
    debug "Sleeping for $CHECK_INTERVAL seconds before next check"
    sleep "$CHECK_INTERVAL"
done
```

This monitoring script demonstrates:

- Continuous service monitoring
- Automatic recovery attempts
- Email notifications
- Command-line parameter handling
- Proper logging of all actions and results


## Part 5: Advanced Logging Techniques

### Structured Logging with JSON

For more complex environments, structured logging in JSON format can be invaluable:

```shellscript
log_json() {
    local level="$1"
    local message="$2"
    local timestamp=$(date -u +"%Y-%m-%dT%H:%M:%S.%3NZ")  # ISO8601 format with milliseconds
    
    # Additional context
    local hostname=$(hostname)
    local script_name=$(basename "$0")
    local pid=$$
    
    # Create JSON log entry
    printf '{"timestamp":"%s","level":"%s","message":"%s","hostname":"%s","script":"%s","pid":%d}\n' \
        "$timestamp" "$level" "$message" "$hostname" "$script_name" "$pid"
}

# Usage
log_json "INFO" "Database backup completed"
```

This produces output like:

```json
{"timestamp":"2025-04-04T16:32:15.123Z","level":"INFO","message":"Database backup completed","hostname":"webserver01","script":"backup.sh","pid":12345}
```

Structured logs are:

- Easily parsable by log management systems
- Searchable and filterable
- Consistent across different scripts and systems


### Capturing Command Output in Logs

To capture both stdout and stderr from commands:

```shellscript
execute_and_log() {
    local cmd="$1"
    local log_prefix="$2"
    local temp_out
    local temp_err
    local exit_code
    
    # Create temporary files for stdout and stderr
    temp_out=$(mktemp)
    temp_err=$(mktemp)
    
    # Execute command, capturing output
    info "$log_prefix command: $cmd"
    eval "$cmd" > "$temp_out" 2> "$temp_err"
    exit_code=$?
    
    # Log stdout if not empty
    if [[ -s "$temp_out" ]]; then
        while IFS= read -r line; do
            debug "$log_prefix stdout: $line"
        done < "$temp_out"
    fi
    
    # Log stderr if not empty
    if [[ -s "$temp_err" ]]; then
        while IFS= read -r line; do
            if [[ $exit_code -eq 0 ]]; then
                debug "$log_prefix stderr: $line"
            else
                warning "$log_prefix stderr: $line"
            fi
        done < "$temp_err"
    fi
    
    # Clean up temp files
    rm -f "$temp_out" "$temp_err"
    
    # Log result
    if [[ $exit_code -eq 0 ]]; then
        info "$log_prefix completed successfully"
    else
        error "$log_prefix failed with exit code $exit_code"
    fi
    
    return $exit_code
}

# Usage
execute_and_log "find /etc -name '*.conf'" "Config search"
```

This function:

- Captures both stdout and stderr separately
- Logs each line with appropriate context
- Handles the exit code appropriately
- Cleans up temporary files


### Log Rotation and Management

For long-running scripts or daemons, implement log rotation:

```shellscript
setup_logging() {
    local log_file="$1"
    local max_size_kb="${2:-1024}"  # Default 1MB
    local backup_count="${3:-5}"    # Default 5 backups
    
    # Check if log file exists and is too large
    if [[ -f "$log_file" && $(du -k "$log_file" | cut -f1) -gt $max_size_kb ]]; then
        info "Rotating log file: $log_file (exceeds $max_size_kb KB)"
        
        # Remove oldest log if we have too many
        if [[ -f "${log_file}.${backup_count}" ]]; then
            rm "${log_file}.${backup_count}"
        fi
        
        # Shift existing logs
        for (( i=backup_count-1; i>=1; i-- )); do
            if [[ -f "${log_file}.${i}" ]]; then
                mv "${log_file}.${i}" "${log_file}.$((i+1))"
            fi
        done
        
        # Move current log to .1
        mv "$log_file" "${log_file}.1"
        
        # Create new log file
        touch "$log_file"
        info "Log rotation completed"
    fi
}

# Usage (call this periodically in long-running scripts)
setup_logging "/var/log/myscript.log" 2048 10
```

This function:

- Checks log file size
- Rotates logs when they exceed a specified size
- Maintains a configurable number of backup logs
- Creates a fresh log file after rotation


## Part 6: Best Practices for Production-Ready Scripts

### 1. Script Security

Secure your scripts with these practices:

```shellscript
# Set restrictive permissions
chmod 700 myscript.sh  # Only owner can read/write/execute

# Use restricted paths for executables
PATH="/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin"

# Avoid using shell variables in eval without proper escaping
unsafe_var="$(malicious command)"
eval "echo $unsafe_var"  # DANGEROUS!

# Instead, use arrays for command construction
cmd=("echo" "$unsafe_var")
"${cmd[@]}"  # SAFE!

# Handle sensitive data carefully
read -s -p "Enter password: " password  # -s hides input
# Don't log passwords or tokens!
```

### 2. Performance Optimization

Make your scripts faster and more efficient:

```shellscript
# Use native bash operations instead of external commands
# Instead of:
count=$(echo "$data" | wc -l)

# Use:
count=$(wc -l <<< "$data")

# Or even better:
count=${#data[@]}  # For arrays

# Minimize subshells
# Instead of:
for file in $(find /path -type f); do

# Use:
while IFS= read -r file; do
    # Process file
done < <(find /path -type f)

# Use built-in string operations
string="Hello World"
length=${#string}
substring=${string:0:5}  # "Hello"
```

### 3. Script Documentation

Document your scripts thoroughly:

```shellscript
#!/bin/bash
#
# Script Name: example.sh
# Description: A comprehensive example script
# Author: Your Name <your.email@example.com>
# Date Created: 2025-04-04
# Last Modified: 2025-04-04
#
# Usage: ./example.sh [OPTIONS]
#
# Options:
#   -h, --help        Display this help message and exit
#   -v, --verbose     Enable verbose output
#   -f, --file FILE   Specify input file
#
# Exit Codes:
#   0 - Success
#   1 - General error
#   2 - Missing dependency
#
# Examples:
#   ./example.sh --file input.txt
#   ./example.sh --verbose
#
# Notes:
#   Requires bash 4.0+ and the following utilities:
#   - jq
#   - curl
#   - rsync

# Function documentation
#######################################
# Processes a file and outputs results
# Globals:
#   VERBOSE - If set, enables verbose output
# Arguments:
#   $1 - Input file path
#   $2 - Output format (text|json)
# Outputs:
#   Writes processing results to stdout
# Returns:
#   0 on success, non-zero on error
#######################################
process_file() {
    local input_file="$1"
    local format="${2:-text}"
    
    # Function implementation...
}
```

### 4. Script Testing

Implement testing for your scripts:

```shellscript
#!/bin/bash
#
# test_backup.sh - Test suite for backup.sh

# Source the script to test its functions
source "./backup.sh"

# Mock functions to avoid actual system changes
mkdir() {
    echo "MOCK: mkdir $*"
    return 0
}

# Test functions
test_check_disk_space() {
    echo "Testing check_disk_space function..."
    
    # Test case 1: Enough space
    if check_disk_space 100; then
        echo "PASS: Correctly detected sufficient space"
    else
        echo "FAIL: Failed to detect sufficient space"
    fi
    
    # More test cases...
}

# Run all tests
run_tests() {
    test_check_disk_space
    # More test functions...
    
    echo "All tests completed"
}

run_tests
```

## Part 7: Troubleshooting Common Bash Script Issues

### 1. Debugging Techniques

When your script isn't working as expected:

```shellscript
# Enable debug mode
set -x  # Print each command before execution
set -v  # Print script lines as they are read

# Trace a specific section
set +x  # Turn off tracing
critical_function() {
    set -x  # Turn on tracing for this function
    # Function code...
    set +x  # Turn off tracing again
}

# Debug variables
debug "Variable values: var1=$var1, var2=$var2"

# Check if commands exist
for cmd in jq curl rsync; do
    if ! command -v "$cmd" &> /dev/null; then
        error "Required command not found: $cmd"
        exit 2
    fi
done
```

### 2. Common Pitfalls and Solutions

```shellscript
# Pitfall: Forgetting to quote variables
file="My Document.txt"
rm $file  # WRONG: expands to 'rm My Document.txt' (two arguments)
rm "$file"  # CORRECT: expands to 'rm "My Document.txt"' (one argument)

# Pitfall: Not checking if files/directories exist
cp "$source_file" "$dest_dir/"  # Fails if source_file doesn't exist
[[ -f "$source_file" ]] && cp "$source_file" "$dest_dir/"  # Better

# Pitfall: Using == instead of = in test constructs
if [ "$var" == "value" ]; then  # Works in bash but not in sh
if [ "$var" = "value" ]; then   # Works in both bash and sh

# Pitfall: Not handling empty variables
log_file=${LOG_FILE:-"/var/log/default.log"}  # Use default if LOG_FILE is empty

# Pitfall: Not handling spaces in filenames
find /path -type f | while read -r file; do  # WRONG
find /path -type f -print0 | while IFS= read -r -d '' file; do  # CORRECT
```

## Part 8: Real-World Applications and Case Studies

### Case Study 1: Automated Database Backup System

A financial services company needed to ensure their database backups were reliable, secure, and properly logged. They implemented a script with:

- Multi-level logging to both files and a centralized logging system
- Encrypted backups with integrity verification
- Automatic rotation and cleanup of old backups
- Email notifications for success/failure
- Detailed logs for audit compliance


The script reduced backup failures by 95% and cut troubleshooting time from hours to minutes.

### Case Study 2: Server Fleet Management

A cloud hosting provider used Bash scripts to manage their server fleet:

- Scripts to provision new servers with consistent configurations
- Monitoring scripts that logged performance metrics and detected anomalies
- Automated remediation scripts that could fix common issues
- Centralized logging that aggregated data from thousands of servers


Their logging system processed over 10GB of log data daily, allowing them to identify patterns and optimize their infrastructure.

### Case Study 3: Continuous Integration Pipeline

A software development team built a CI pipeline using Bash scripts:

- Scripts to build and test code on each commit
- Detailed logging of each build step
- Automatic notification of build failures
- Performance metrics for build times


The structured logs allowed them to identify bottlenecks in their build process and reduce build times by 40%.

## Part 9: Advanced Exercises and Challenges

### Exercise 1: Build a Comprehensive Logging Library

Create a reusable logging library that includes:

- Multiple output destinations (console, file, syslog, HTTP endpoint)
- Log rotation with configurable retention policies
- Structured logging with JSON support
- Log level filtering
- Performance metrics (execution time, memory usage)


```shellscript
#!/bin/bash
#
# advanced_logging.sh - A comprehensive logging library
# Usage: source ./advanced_logging.sh

# Configuration variables with defaults
LOG_LEVEL=${LOG_LEVEL:-"INFO"}
LOG_FILE=${LOG_FILE:-""}
LOG_FORMAT=${LOG_FORMAT:-"text"}  # text or json
LOG_SYSLOG=${LOG_SYSLOG:-false}
LOG_HTTP_ENDPOINT=${LOG_HTTP_ENDPOINT:-""}
LOG_HTTP_TOKEN=${LOG_HTTP_TOKEN:-""}

# Log levels
declare -A LOG_LEVELS=( 
    ["DEBUG"]=0
    ["INFO"]=1
    ["WARNING"]=2
    ["ERROR"]=3
    ["CRITICAL"]=4
)

# ANSI color codes
readonly COLOR_RESET="\033[0m"
readonly COLOR_RED="\033[0;31m"
readonly COLOR_GREEN="\033[0;32m"
readonly COLOR_YELLOW="\033[0;33m"
readonly COLOR_BLUE="\033[0;34m"
readonly COLOR_PURPLE="\033[0;35m"

# Initialize logging
init_logging() {
    # Validate log level
    if [[ -z "${LOG_LEVELS[$LOG_LEVEL]}" ]]; then
        echo "Invalid log level: $LOG_LEVEL. Using INFO."
        LOG_LEVEL="INFO"
    fi
    
    # Create log file directory if needed
    if [[ -n "$LOG_FILE" ]]; then
        mkdir -p "$(dirname "$LOG_FILE")" || {
            echo "Failed to create log directory: $(dirname "$LOG_FILE")"
            LOG_FILE=""
        }
    fi
    
    # Test syslog if enabled
    if [[ "$LOG_SYSLOG" == true ]]; then
        if ! command -v logger &> /dev/null; then
            echo "Logger command not found, disabling syslog output."
            LOG_SYSLOG=false
        fi
    fi
    
    # Test HTTP endpoint if configured
    if [[ -n "$LOG_HTTP_ENDPOINT" ]]; then
        if ! command -v curl &> /dev/null; then
            echo "Curl command not found, disabling HTTP logging."
            LOG_HTTP_ENDPOINT=""
        fi
    fi
    
    # Log initialization
    _log "INFO" "Logging initialized: level=$LOG_LEVEL, format=$LOG_FORMAT"
}

# Internal logging function
_log() {
    local level="$1"
    local message="$2"
    local timestamp=$(date '+%Y-%m-%d %H:%M:%S.%3N')
    local script_name=$(basename "$0")
    local pid=$$
    
    # Check if we should log this level
    if [[ "${LOG_LEVELS[$level]}" -lt "${LOG_LEVELS[$LOG_LEVEL]}" ]]; then
        return 0
    fi
    
    # Format the log entry
    local formatted_log
    if [[ "$LOG_FORMAT" == "json" ]]; then
        # Escape quotes in message
        local escaped_message="${message//\"/\\\"}"
        formatted_log="{\"timestamp\":\"$timestamp\",\"level\":\"$level\",\"message\":\"$escaped_message\",\"script\":\"$script_name\",\"pid\":$pid}"
    else
        # Text format
        formatted_log="[$timestamp] [$level] [$script_name:$pid] $message"
    fi
    
    # Console output with colors
    if [[ "$LOG_FORMAT" != "json" ]]; then
        case "$level" in
            "DEBUG")
                echo -e "[$timestamp] ${COLOR_BLUE}[$level]${COLOR_RESET} [$script_name:$pid] $message"
                ;;
            "INFO")
                echo -e "[$timestamp] ${COLOR_GREEN}[$level]${COLOR_RESET} [$script_name:$pid] $message"
                ;;
            "WARNING")
                echo -e "[$timestamp] ${COLOR_YELLOW}[$level]${COLOR_RESET} [$script_name:$pid] $message"
                ;;
            "ERROR")
                echo -e "[$timestamp] ${COLOR_RED}[$level]${COLOR_RESET} [$script_name:$pid] $message"
                ;;
            "CRITICAL")
                echo -e "[$timestamp] ${COLOR_PURPLE}[$level]${COLOR_RESET} [$script_name:$pid] $message"
                ;;
        esac
    else
        echo "$formatted_log"
    fi
    
    # File output
    if [[ -n "$LOG_FILE" ]]; then
        echo "$formatted_log" >> "$LOG_FILE"
    fi
    
    # Syslog output
    if [[ "$LOG_SYSLOG" == true ]]; then
        case "$level" in
            "DEBUG")
                logger -p user.debug -t "$script_name" "$message"
                ;;
            "INFO")
                logger -p user.info -t "$script_name" "$message"
                ;;
            "WARNING")
                logger -p user.warning -t "$script_name" "$message"
                ;;
            "ERROR")
                logger -p user.err -t "$script_name" "$message"
                ;;
            "CRITICAL")
                logger -p user.crit -t "$script_name" "$message"
                ;;
        esac
    fi
    
    # HTTP endpoint
    if [[ -n "$LOG_HTTP_ENDPOINT" ]]; then
        # Send asynchronously to not block the script
        (
            curl -s -X POST "$LOG_HTTP_ENDPOINT" \
                -H "Content-Type: application/json" \
                -H "Authorization: Bearer $LOG_HTTP_TOKEN" \
                -d "$formatted_log" &> /dev/null
        ) &
    fi
}

# Public logging functions
log_debug() { _log "DEBUG" "$1"; }
log_info() { _log "INFO" "$1"; }
log_warning() { _log "WARNING" "$1"; }
log_error() { _log "ERROR" "$1"; }
log_critical() { _log "CRITICAL" "$1"; }

# Rotate log file if needed
rotate_log() {
    local max_size_kb="${1:-1024}"
    local backup_count="${2:-5}"
    
    if [[ -z "$LOG_FILE" || ! -f "$LOG_FILE" ]]; then
        return 0
    fi
    
    local size_kb=$(du -k "$LOG_FILE" | cut -f1)
    
    if [[ $size_kb -gt $max_size_kb ]]; then
        log_info "Rotating log file: $LOG_FILE (size: ${size_kb}KB, limit: ${max_size_kb}KB)"
        
        # Remove oldest log if we have too many
        if [[ -f "${LOG_FILE}.${backup_count}" ]]; then
            rm "${LOG_FILE}.${backup_count}"
        fi
        
        # Shift existing logs
        for (( i=backup_count-1; i>=1; i-- )); do
            if [[ -f "${LOG_FILE}.${i}" ]]; then
                mv "${LOG_FILE}.${i}" "${LOG_FILE}.$((i+1))"
            fi
        done
        
        # Move current log to .1
        mv "$LOG_FILE" "${LOG_FILE}.1"
        touch "$LOG_FILE"
        
        log_info "Log rotation completed"
    fi
}

# Initialize logging on source
init_logging
```

### Exercise 2: Create a System Health Monitoring Script

Build a script that:

1. Monitors CPU, memory, disk, and network usage
2. Logs metrics in a structured format
3. Alerts when thresholds are exceeded
4. Generates daily reports
5. Implements proper error handling and logging


```shellscript
#!/bin/bash
#
# system_monitor.sh - Comprehensive system health monitoring
# Usage: ./system_monitor.sh [--interval=60] [--config=/path/to/config.conf]

# Source our advanced logging library
source "./advanced_logging.sh"

# Default configuration
INTERVAL=60  # seconds
CONFIG_FILE="/etc/system_monitor.conf"
REPORT_DIR="/var/log/system_monitor"
ALERT_EMAIL=""

# Thresholds
CPU_THRESHOLD=80
MEMORY_THRESHOLD=80
DISK_THRESHOLD=85
LOAD_THRESHOLD=$(nproc)  # Number of CPU cores

# Parse command line arguments
for arg in "$@"; do
    case $arg in
        --interval=*)
            INTERVAL="${arg#*=}"
            ;;
        --config=*)
            CONFIG_FILE="${arg#*=}"
            ;;
        --help)
            echo "Usage: $0 [--interval=60] [--config=/path/to/config.conf]"
            exit 0
            ;;
    esac
done

# Load configuration if exists
if [[ -f "$CONFIG_FILE" ]]; then
    log_info "Loading configuration from $CONFIG_FILE"
    source "$CONFIG_FILE"
else
    log_warning "Configuration file not found: $CONFIG_FILE"
fi

# Create report directory
mkdir -p "$REPORT_DIR" || {
    log_critical "Failed to create report directory: $REPORT_DIR"
    exit 1
}

# Function to collect CPU metrics
collect_cpu_metrics() {
    local cpu_idle
    local cpu_usage
    
    # Get CPU idle percentage
    cpu_idle=$(top -bn1 | grep "Cpu(s)" | awk '{print $8}' | tr -d '%')
    cpu_usage=$(awk "BEGIN {print 100 - $cpu_idle}")
    
    echo "$cpu_usage"
}

# Function to collect memory metrics
collect_memory_metrics() {
    local mem_total
    local mem_available
    local mem_usage_percent
    
    # Get memory usage
    mem_total=$(free -m | awk '/Mem:/ {print $2}')
    mem_available=$(free -m | awk '/Mem:/ {print $7}')
    mem_usage_percent=$(awk "BEGIN {print 100 - ($mem_available / $mem_total * 100)}")
    
    echo "$mem_usage_percent"
}

# Function to collect disk metrics
collect_disk_metrics() {
    local disk_usage
    
    # Get disk usage percentage for root filesystem
    disk_usage=$(df -h / | awk 'NR==2 {print $5}' | tr -d '%')
    
    echo "$disk_usage"
}

# Function to collect load average
collect_load_metrics() {
    local load_avg
    
    # Get 1-minute load average
    load_avg=$(uptime | awk -F'[a-z]:' '{print $2}' | awk '{print $1}' | tr -d ',')
    
    echo "$load_avg"
}

# Function to collect network metrics
collect_network_metrics() {
    local rx_bytes
    local tx_bytes
    
    # Get network statistics for primary interface
    primary_interface=$(ip route | grep default | awk '{print $5}')
    rx_bytes=$(cat /proc/net/dev | grep "$primary_interface" | awk '{print $2}')
    tx_bytes=$(cat /proc/net/dev | grep "$primary_interface" | awk '{print $10}')
    
    echo "$rx_bytes $tx_bytes"
}

# Function to send alert
send_alert() {
    local subject="$1"
    local message="$2"
    
    log_warning "ALERT: $subject - $message"
    
    if [[ -n "$ALERT_EMAIL" ]]; then
        echo -e "$message" | mail -s "SYSTEM ALERT: $subject" "$ALERT_EMAIL"
    fi
}

# Function to generate daily report
generate_daily_report() {
    local report_file="$REPORT_DIR/report_$(date +%Y%m%d).txt"
    local json_file="$REPORT_DIR/report_$(date +%Y%m%d).json"
    
    log_info "Generating daily report: $report_file"
    
    # Text report
    {
        echo "=== SYSTEM HEALTH REPORT ==="
        echo "Date: $(date)"
        echo "Hostname: $(hostname)"
        echo ""
        echo "=== CPU USAGE ==="
        echo "Current: $(collect_cpu_metrics)%"
        echo "Average (24h): $(awk '{sum+=$1; count++} END {print sum/count}' "$REPORT_DIR/cpu_$(date +%Y%m%d).log")%"
        echo ""
        echo "=== MEMORY USAGE ==="
        echo "Current: $(collect_memory_metrics)%"
        echo "Average (24h): $(awk '{sum+=$1; count++} END {print sum/count}' "$REPORT_DIR/memory_$(date +%Y%m%d).log")%"
        echo ""
        echo "=== DISK USAGE ==="
        echo "Current: $(collect_disk_metrics)%"
        echo ""
        echo "=== LOAD AVERAGE ==="
        echo "Current: $(collect_load_metrics)"
        echo "Average (24h): $(awk '{sum+=$1; count++} END {print sum/count}' "$REPORT_DIR/load_$(date +%Y%m%d).log")"
        echo ""
        echo "=== TOP PROCESSES BY CPU ==="
        ps aux --sort=-%cpu | head -11
        echo ""
        echo "=== TOP PROCESSES BY MEMORY ==="
        ps aux --sort=-%mem | head -11
    } > "$report_file"
    
    # JSON report
    {
        echo "{"
        echo "  \"timestamp\": \"$(date -Iseconds)\","
        echo "  \"hostname\": \"$(hostname)\","
        echo "  \"cpu\": {"
        echo "    \"current\": $(collect_cpu_metrics),"
        echo "    \"average\": $(awk '{sum+=$1; count++} END {print sum/count}' "$REPORT_DIR/cpu_$(date +%Y%m%d).log")"
        echo "  },"
        echo "  \"memory\": {"
        echo "    \"current\": $(collect_memory_metrics),"
        echo "    \"average\": $(awk '{sum+=$1; count++} END {print sum/count}' "$REPORT_DIR/memory_$(date +%Y%m%d).log")"
        echo "  },"
        echo "  \"disk\": {"
        echo "    \"current\": $(collect_disk_metrics)"
        echo "  },"
        echo "  \"load\": {"
        echo "    \"current\": $(collect_load_metrics),"
        echo "    \"average\": $(awk '{sum+=$1; count++} END {print sum/count}' "$REPORT_DIR/load_$(date +%Y%m%d).log")"
        echo "  }"
        echo "}"
    } > "$json_file"
    
    log_info "Daily report generated"
}

# Main monitoring loop
log_info "Starting system monitoring (interval: ${INTERVAL}s)"

# Create daily log files
cpu_log="$REPORT_DIR/cpu_$(date +%Y%m%d).log"
memory_log="$REPORT_DIR/memory_$(date +%Y%m%d).log"
disk_log="$REPORT_DIR/disk_$(date +%Y%m%d).log"
load_log="$REPORT_DIR/load_$(date +%Y%m%d).log"

touch "$cpu_log" "$memory_log" "$disk_log" "$load_log"

# Track the current day for log rotation
current_day=$(date +%d)

while true; do
    # Check if day has changed for log rotation
    if [[ $(date +%d) != "$current_day" ]]; then
        log_info "Day changed, generating final report for previous day"
        generate_daily_report
        
        # Update current day
        current_day=$(date +%d)
        
        # Create new log files
        cpu_log="$REPORT_DIR/cpu_$(date +%Y%m%d).log"
        memory_log="$REPORT_DIR/memory_$(date +%Y%m%d).log"
        disk_log="$REPORT_DIR/disk_$(date +%Y%m%d).log"
        load_log="$REPORT_DIR/load_$(date +%Y%m%d).log"
        
        touch "$cpu_log" "$memory_log" "$disk_log" "$load_log"
    fi
    
    # Collect metrics
    cpu_usage=$(collect_cpu_metrics)
    memory_usage=$(collect_memory_metrics)
    disk_usage=$(collect_disk_metrics)
    load_avg=$(collect_load_metrics)
    network_stats=$(collect_network_metrics)
    
    # Log metrics
    echo "$(date +%s) $cpu_usage" >> "$cpu_log"
    echo "$(date +%s) $memory_usage" >> "$memory_log"
    echo "$(date +%s) $disk_usage" >> "$disk_log"
    echo "$(date +%s) $load_avg" >> "$load_log"
    
    # Log in structured format
    if [[ "$LOG_FORMAT" == "json" ]]; then
        log_info "{\"cpu\":$cpu_usage,\"memory\":$memory_usage,\"disk\":$disk_usage,\"load\":$load_avg}"
    else
        log_info "Metrics - CPU: ${cpu_usage}%, Memory: ${memory_usage}%, Disk: ${disk_usage}%, Load: $load_avg"
    fi
    
    # Check thresholds and alert if needed
    if (( $(echo "$cpu_usage > $CPU_THRESHOLD" | bc -l) )); then
        send_alert "High CPU Usage" "CPU usage is at ${cpu_usage}% (threshold: ${CPU_THRESHOLD}%)"
    fi
    
    if (( $(echo "$memory_usage > $MEMORY_THRESHOLD" | bc -l) )); then
        send_alert "High Memory Usage" "Memory usage is at ${memory_usage}% (threshold: ${MEMORY_THRESHOLD}%)"
    fi
    
    if (( $(echo "$disk_usage > $DISK_THRESHOLD" | bc -l) )); then
        send_alert "High Disk Usage" "Disk usage is at ${disk_usage}% (threshold: ${DISK_THRESHOLD}%)"
    fi
    
    if (( $(echo "$load_avg > $LOAD_THRESHOLD" | bc -l) )); then
        send_alert "High System Load" "Load average is at $load_avg (threshold: $LOAD_THRESHOLD)"
    fi
    
    # Sleep until next check
    sleep "$INTERVAL"
done
```

### Exercise 3: Build an Automated Deployment Script

Create a script that:

1. Deploys an application to multiple servers
2. Implements rolling deployments to avoid downtime
3. Includes comprehensive logging and error handling
4. Provides rollback capability if deployment fails
5. Notifies stakeholders of deployment status


## Part 10: Resources for Further Learning

### Books

1. **"Bash Cookbook: Solutions and Examples for Bash Users"** by Carl Albing, JP Vossen, and Cameron Newham
2. **"Linux Command Line and Shell Scripting Bible"** by Richard Blum and Christine Bresnahan
3. **"Mastering Linux Shell Scripting"** by Mokhtar Ebrahim and Andrew Mallett


### Online Resources

1. **Bash Documentation**: The official Bash manual (`man bash`)
2. **Advanced Bash-Scripting Guide**: A comprehensive guide to shell scripting
3. **ShellCheck**: An online tool for analyzing shell scripts
4. **Explainshell**: A web tool that explains shell commands
5. **Stack Overflow Bash Tag**: A wealth of real-world problems and solutions


### Practice Platforms

1. **HackerRank Shell Track**: Exercises to improve your shell scripting skills
2. **Exercism Bash Track**: Mentored coding challenges
3. **Linux Academy**: Hands-on labs for shell scripting


## Conclusion: The Path to Bash Mastery

Mastering Bash scripting and logging is a journey, not a destination. As you continue to develop your skills, remember these key principles:

1. **Readability First**: Write scripts that humans can understand
2. **Log Everything**: Your future self will thank you
3. **Handle Errors Gracefully**: Anticipate and manage failures
4. **Test Thoroughly**: Verify your scripts work in all scenarios
5. **Document Clearly**: Help others (and yourself) understand your work


The skills you've learned today form the foundation of professional Linux engineering. By implementing proper logging and error handling in your scripts, you're not just writing code—you're building reliable, maintainable tools that you and your team can depend on.

As you progress through this 30-day training series, you'll build on these fundamentals to tackle increasingly complex challenges. Each day's learning will compound, giving you the confidence and capability to handle real-world Linux engineering tasks with professionalism and skill.

Remember: The difference between a script that works and a script that works reliably in production often comes down to the quality of its logging and error handling. Invest time in these fundamentals, and you'll reap the benefits throughout your career.

Happy scripting!
